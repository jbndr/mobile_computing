\chapter{Einleitung}
\label{cha:einleitung}

In dieser Arbeit werden die zentralen Eigenschaften von Microservices im Zusammenhang mit ihrer Skalierfähigkeit untersucht.
Im Mittelpunkt wird die Verteilung der Microservices behandelt und die gewonnen Kenntnisse sowohl in der Infrastruktur der Forschungsgruppe als auch der Infrastruktur des Kooperationspartners analysiert.
Des Weiteren wird auf den Entwicklungsprozess eingegangen, sodass die Skalierbarkeit der Microservices optimiert wird.

\section{Einführung in das Themengebiet}

\todo{Mehr auf die Verteilung der Services eingehen?}
\todo{Die CI/CD Pipeline einbeziehen?}

Onlinedienste wie bspw. Netflix, Spotify und Zalando möchten ihren Kunden eine globale und stetige Erreichbarkeit ihrer Anwendung bereitstellen.
Dies hat zur Folge, dass Nutzer solcher Onlinedienste eine hohe Erwartungshaltung gegenüber ebendieser Eigenschaften haben.
Die Dienste müssen in der Lage sein mehrere Millionen Nutzeranfragen bearbeiten zu können, ohne, dass dies zu einem Ausfall der Anwendung führt.
Im Mittelpunkt steht dabei die Skalierbarkeit der Anwendung, sodass die Ausfallzeit eines Services minimiert werden kann.

Insbesondere hat die Evolution der Softwarearchitektur von einer monolithischen Architektur zur Microservice-Architektur den Prozess der Skalierbarkeit angetrieben.
Neben den oben genannten Unternehmen setzen Amazon, Sound Cloud und Ebay ebenfalls auf die Microservice-Architektur, um die hohe Anzahl an Nutzeranfragen bearbeiten zu können.
Einer der Hauptvorteile ist, dass Microservices wiederverwendbare Funktionalität kapseln und dadurch unabhängig voneinander skaliert werden können.
Wird ein bestimmter Microservice einer Web-Anwendung stärker ausgelastet, so kann darauf reagiert werden und durch eine Anpassung der Infrastruktur der Microservice unterstützt werden.
Auf der einen Seite kann ein Rechner durch das Hinzufügen von Ressourcen, wie bspw. Arbeitsspeicher oder einer Festplatte, vertikal skaliert werden.
Auf der anderen Seite können weitere Rechner dem System hinzugefügt werden und somit die Auslastung des Microservices gesteuert werden.

Bei der Einhaltung der geforderten Eigenschaften nehmen die Werkzeuge Docker und Kubernetes eine zentrale Rolle ein.
Dabei ermöglicht Docker die Isolation des Microservices von der Hardware und sichert durch die Verwaltung von Abhängigkeiten einen konsistenten Zustand, der auf jeder Maschine hergestellt werden kann. 
Kubernetes dient zur Orchestrierung der Docker-Container und abstrahiert diese.
Dadurch ist es Kubernetes möglich, die Container selbst zu verwalten und bildet somit den integralen Bestandteil zur Skalierung von Microservices.
Die Werkzeuge ermöglichen nicht nur die Skalierung eines Microserivces, sondern auch dessen Verteilung.

Schwerpunkt dieser Arbeit ist die Bewertung von Microservices im Hinblick auf die Skalierbarkeit und die Ermittlung der wesentlichen Faktoren, die bereits im Entwicklungsprozess einen Einfluss auf die Skalierfähigkeit des Mircoservices haben.

\section{Fragestellungen}

Im Rahmen dieser Arbeit sollen die folgenden Fragen beantwortet werden.

\begin{enumerate}
	\item \textbf{Wie sieht die aktuelle Infrastruktur der Forschungsgruppe aus?}
    \item \textbf{Wie lässt sich die bestehende TLM-Anwendung in der Kubernetes-basierten Infrastruktur bereitstellen?}
	\item \textbf{Welche Auswirkungen auf die Verteilung und Skalierbarkeit hat die neue Infrastruktur?}
	\item \textbf{Welche Eigenschaften haben einen Einfluss auf die Skalierfähigkeit der TLM-Anwendung?}
	\item \textbf{Inwiefern kann der Entwicklungsprozess angepasst werden, damit die Skalierbarkeit verbessert wird?}
\end{enumerate}

\section{Beschreibung des Demonstrators}

Teil dieser Arbeit ist die Erarbeitung eines Konzepts, welches die Verteilung von Microservices in einer Kubernetes-basierten Infrastruktur realisiert.
Zur Demonstration des Konzepts wird ein bei C\&M entwickelter Microservice, eine TodoListManagement-Anwendung, verwendet.

Wesentlich für die automatisierte Verteilung von Microservices ist eine CI/CD-Pipeline.
Continious Integration beschreibt die kontinuierliche Integration des Codes in einem Repository.
Dies bietet die Möglichkeit ausführbare Artefakte automatisch zu erzeugen.
Continious Deployment ist eine Erweiterung des Konzepts der Continious Integration und beschreibt die automatisierte Verteilung der Anwendung.
Dadurch wird sichergestellt, dass Änderungen an der Software immer einen Zustand einhalten, welcher ausgeliefert werden kann.

Auch das Konzept der Kubernetes-basierten Infrastruktur, welche in der Abbildung \ref{fig:demonstrator} illustiert wird, basiert auf einer CI/CD-Pipeline.
Die Infrastruktur besteht aus drei zentralen Komponenten: den Entwicklern, einem Continious Integration Server und einem Deployment Cluster.

Eine notwendige Bedingung für den Start der CI/CD-Pipeline ist das Ablegen der Entwicklungsergebnisse in einem verteilten Repository.

Der Continious Integration Server (CI-Server) identifiziert eine Änderung im Repository und stößt den weiteren Prozess an.
Es erfolgt das Testen der Anwendung sowie die Erzeugung eines ausführbaren Artefakts.
Anschließend wird ein Docker Image, welche das ausführbare Artefakt vom Betriebsystem abstrahiert, erzeugt.
Das Docker Image wird in einer privaten Docker Registry hochgeladen.
Die Docker Registry bietet eine Sammlung von Docker Images, sodass diese ohne großen Aufwand wiederverwendet werden können.
Im nächsten Schritt wird die Verteilung der Anwendung vorbereitet. 
Die Anwendung wird mithilfe des Werkzeugs Helm, einer Paketverwaltungsanwendung für Kubernetes, im Kubernetes Cluster installiert.

Die abschließende Komponente der CI/CD-Pipeline stellt das Deployment Cluster dar.
Das Kubernetes Cluster besteht aus verschiedenen Knoten, die entweder eine Maschine oder virtuelle Maschine sein können, die miteinander kommunizieren.
Der Hauptvorteil von Kubernetes ist dabei die Einhaltung eines geforderten Ist-Zustands.
Dies ist die zentrale Grundlage für eine hohe Verfügbarkeit der Anwendung und ermöglicht die Skalierung von Microservices.
Der Zustand des Clusters wird durch ein automatisch verteiltes Monitoring überwacht und kann zur Alarmierung eines kritischen Systemzustands verwendet werden.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/demonstrator.png}
	\caption{Einsatz von Kubernetes bei C\&M}
	\label{fig:demonstrator}
\end{figure}

\section{Gliederung der Arbeit}
\label{sec:gliederung}

In Kapitel 2 werden die Grundlagen, die für das Verständnis der Inhalte der Arbeit erforderlich sind, vorgestellt.
Anschließend wird in Kapitel 3 der Stand der Technik vorgestellt.
Das nächste Kapitel ...

\todo{Inhaltskapitel überlegen}

\newpage
\subsection*{Kapitel 2: Grundlagen}

Dieses Kapitel beinhaltet Informationen, die zum grundlegenden Verständnis der Inhalte der Arbeit erforderlich sind. Die Informationen werden weitestgehend "wertfrei" darge-stellt. Beispiele sind:...

\subsection*{Kapitel 3: Stand der Technik}

In dieses Kapitel fließen ganz wesentlich die Ergebnisse der Literaturanalyse ein. Es bietet sich an, die in der Einleitung eingeführte Struktur hier wieder aufzugreifen.

Im Gegensatz zu Kapitel 2 werden die Inhalte dieses Kapitels in einer argumentativen und wertenden Form dargestellt.

\iffalse
\subsection{Latex-Vorlagen}

\subsection{Zitate}
\label{subsec:zitate}
Der Demonstrator nutzt meist Komponenten, die bereit sin früheren Arbeiten bei \gls{CM} entwickelt wurden.
\begin{quote}
\textit{``A microservice is a cohesive, independent process interacting via messages``}
\end{quote}
\begin{quote}
\textit{``fictive book quote.'', \cite[S.~99]{Be02}}
\end{quote}

\subsection{Grafiken}
Einfügen von Grafiken und referenzieren der Abbildung \ref{fig:lehre}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/lehre_kreislauf.png}
	\caption{Lehre-Forschung \& Praxis-Kreislauf}
	\label{fig:lehre}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{ | l | p{7cm} | }
		\hline
		ServiceGroup & Dienstleistung \\
		\hline
		 CoreServiceGroup & Campus-Informationen und Navigation \\
	 	\hline
	 	SCbInfoServiceGroup (SCbSG) & Hörsaal-Informationen für Studierende mit Beeinträchtigung \\
	 	\hline
	 	 StudAdviceTicket (StuSG) & Studierendenberatungs-Anmeldung \\
	 	\hline
	 	WorkspaceServiceGroup (WorSG) & Arbeitsplatz-Suche und -Reservierung \\
	 	\hline
	 	BaMaThesisAdminSG (BaMSG) & Abschlussarbeitenverwaltung \\
	 	\hline
	\end{tabular}
	\caption{ServiceGroups in SmartCampus}
	\label{tab:smartcampus-servicegroups}
\end{table}

\subsection{Quellcode}
\vspace{0.5cm}
\begin{lstlisting}[caption = {Vorlage für eine Story und Szenarien nach BDD}, label = {lst:bdd-stories-szenarien-template}, style = kit-cm, language = Gherkin]
Title (one line describing the story)
 
Narrative:
As a [role]
I want [feature]
So that [benefit]
 
Acceptance Criteria: (presented as Scenarios)
 
Scenario 1: Title
Given [context]
  And [some more context]...
When  [event]
Then  [outcome]
  And [another outcome]...
 
Scenario 2: ...
\end{lstlisting}
\fi

